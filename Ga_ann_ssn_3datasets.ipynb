{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8nHTcRc9HGf"
      },
      "source": [
        "# Unified GA-ANN-SSN for Multiple Datasets\n",
        "## Genetic Algorithm with Artificial Neural Network and Structured Sparsity Norm\n",
        "\n",
        "### Supported Datasets:\n",
        "- **Plants Dataset**: Pre-split Train/Validation/Test folders\n",
        "- **AID Dataset**: 70-10-20 split (needs splitting)\n",
        "- **LC25000 Dataset**: Custom Test set + Train-Validation set (80-20 split)\n",
        "\n",
        "### Workflow:\n",
        "1. Dataset selection and configuration\n",
        "2. Automatic download and data loading\n",
        "3. VGG16 feature extraction\n",
        "4. PCA dimensionality reduction\n",
        "5. GA-ANN-SSN feature selection\n",
        "6. Final model training and evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfEURUni9HGk"
      },
      "source": [
        "## Cell 1: Installations and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZeY2pfL9HGl",
        "outputId": "b6eefcb2-2455-426b-abef-62b4db1924f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ All imports successful!\n"
          ]
        }
      ],
      "source": [
        "# Cell 1: Installations and Imports\n",
        "# =================================\n",
        "!pip install opendatasets kaggle pandas tensorflow scikit-learn opencv-python -q\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import random\n",
        "import gc\n",
        "import pickle\n",
        "import json\n",
        "import opendatasets as od\n",
        "from glob import glob\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import accuracy_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore', category=UserWarning, module='keras.*')\n",
        "\n",
        "print(\"✅ All imports successful!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLCWoTmf9HGn"
      },
      "source": [
        "## Cell 2: Configuration and Dataset Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ir9T4T999HGn",
        "outputId": "5c3f325d-c9ca-46a7-af96-8dc4fe54936a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available datasets:\n",
            "1. plants\n",
            "2. aid\n",
            "3. lc25000\n",
            "Select dataset (enter number or name): 3\n",
            "Selected dataset: lc25000\n",
            "Are you running this code in Google Colab or locally? (colab/local): colab\n",
            "✅ Configuration set for LC25000 dataset!\n"
          ]
        }
      ],
      "source": [
        "# Cell 2: Configuration and Dataset Selection\n",
        "# ===========================================\n",
        "def get_dataset_choice():\n",
        "    print(\"Available datasets:\")\n",
        "    print(\"1. plants\")\n",
        "    print(\"2. aid\")\n",
        "    print(\"3. lc25000\")\n",
        "\n",
        "    while True:\n",
        "        choice = input(\"Select dataset (enter number or name): \").strip().lower()\n",
        "\n",
        "        # Handle numeric input\n",
        "        if choice == \"1\":\n",
        "            return \"plants\"\n",
        "        elif choice == \"2\":\n",
        "            return \"aid\"\n",
        "        elif choice == \"3\":\n",
        "            return \"lc25000\"\n",
        "        elif choice in [\"plants\", \"aid\", \"lc25000\"]:\n",
        "            return choice\n",
        "        else:\n",
        "            print(\"Invalid choice. Please enter 1, 2, 3, or the dataset name.\")\n",
        "\n",
        "DATASET_CHOICE = get_dataset_choice()\n",
        "print(f\"Selected dataset: {DATASET_CHOICE}\")\n",
        "\n",
        "#Options: \"plants\", \"aid\", \"lc25000\"\n",
        "#DATASET_CHOICE = \"aid\"\n",
        "\n",
        "\n",
        "import os\n",
        "\n",
        "# Ask user about their environment\n",
        "def get_environment():\n",
        "    while True:\n",
        "        env = input(\"Are you running this code in Google Colab or locally? (colab/local): \").lower().strip()\n",
        "        if env in ['colab', 'local']:\n",
        "            return env\n",
        "        else:\n",
        "            print(\"Please enter either 'colab' or 'local'\")\n",
        "\n",
        "environment = get_environment()\n",
        "\n",
        "# Dataset-specific configurations\n",
        "DATASET_CONFIGS = {\n",
        "    \"plants\": {\n",
        "        \"dataset_url\": \"https://www.kaggle.com/datasets/yudhaislamisulistya/plants-type-datasets\",\n",
        "        \"download_folder\": \"./plant-datasets\",\n",
        "        \"train_path\": \"\",\n",
        "        \"val_path\": \"\",\n",
        "        \"test_path\": \"\",\n",
        "        \"features_dir\": \"./saved_features_plants\",\n",
        "        \"split_type\": \"pre_split\"\n",
        "    },\n",
        "    \"aid\": {\n",
        "        \"dataset_url\": \"https://www.kaggle.com/datasets/jiayuanchengala/aid-scene-classification-datasets\",\n",
        "        \"download_folder\": \"./aid-datasets\",\n",
        "        \"data_root\": \"\",\n",
        "        \"features_dir\": \"./saved_features_aid\",\n",
        "        \"split_type\": \"70_10_20\"\n",
        "    },\n",
        "    \"lc25000\": {\n",
        "        \"dataset_url\": \"https://www.kaggle.com/datasets/javaidahmadwani/lc25000\",\n",
        "        \"download_folder\": \"./lc25000-datasets\",\n",
        "        \"data_root\": \"\",\n",
        "        \"features_dir\": \"./saved_features_lc25000\",\n",
        "        \"split_type\": \"lc25000_split\"\n",
        "    }\n",
        "}\n",
        "\n",
        "# Set paths based on environment\n",
        "if environment == \"colab\":\n",
        "    # Colab paths\n",
        "    DATASET_CONFIGS[\"plants\"][\"train_path\"] = \"/content/plant-datasets/plants-type-datasets/split_ttv_dataset_type_of_plants/Train_Set_Folder\"\n",
        "    DATASET_CONFIGS[\"plants\"][\"val_path\"] = \"/content/plant-datasets/plants-type-datasets/split_ttv_dataset_type_of_plants/Validation_Set_Folder\"\n",
        "    DATASET_CONFIGS[\"plants\"][\"test_path\"] = \"/content/plant-datasets/plants-type-datasets/split_ttv_dataset_type_of_plants/Test_Set_Folder\"\n",
        "    DATASET_CONFIGS[\"aid\"][\"data_root\"] = \"./aid-datasets/aid-scene-classification-datasets/AID\"\n",
        "    DATASET_CONFIGS[\"lc25000\"][\"data_root\"] = \"./lc25000-datasets/lc25000/lung_colon_image_set\"\n",
        "\n",
        "else:  # local environment\n",
        "    print(\"\\nFor local execution, please ensure you have the following folders in your current directory:\")\n",
        "    print(\"1. split_ttv_dataset_type_of_plants\")\n",
        "    print(\"2. AID\")\n",
        "    print(\"3. lung_colon_image_set\")\n",
        "\n",
        "\n",
        "    # Local paths\n",
        "    DATASET_CONFIGS[\"plants\"][\"train_path\"] = \"./split_ttv_dataset_type_of_plants/Train_Set_Folder\"\n",
        "    DATASET_CONFIGS[\"plants\"][\"val_path\"] = \"./split_ttv_dataset_type_of_plants/Validation_Set_Folder\"\n",
        "    DATASET_CONFIGS[\"plants\"][\"test_path\"] = \"./split_ttv_dataset_type_of_plants/Test_Set_Folder\"\n",
        "    DATASET_CONFIGS[\"aid\"][\"data_root\"] = \"./AID\"\n",
        "    DATASET_CONFIGS[\"lc25000\"][\"data_root\"] = \"./lung_colon_image_set\"\n",
        "\n",
        "\n",
        "# Get current config\n",
        "CONFIG = DATASET_CONFIGS[DATASET_CHOICE]\n",
        "\n",
        "# Common parameters\n",
        "IMG_SIZE = (128, 128)\n",
        "VGG_TARGET_SIZE = (224, 224)\n",
        "PCA_VARIANCE = 0.95\n",
        "\n",
        "# GA Parameters\n",
        "GA_POPULATION_SIZE = 10\n",
        "GA_MAX_GENERATION = 40\n",
        "GA_CROSSOVER_RATE = 0.8\n",
        "GA_MUTATION_RATE = 0.2\n",
        "\n",
        "# Training Parameters\n",
        "FEATURE_EXTRACTION_BATCH_SIZE = 128\n",
        "BATCH_SIZE = 64\n",
        "EVAL_EPOCHS = 10\n",
        "FINAL_EPOCHS = 50\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "# Set seeds\n",
        "np.random.seed(RANDOM_SEED)\n",
        "random.seed(RANDOM_SEED)\n",
        "tf.random.set_seed(RANDOM_SEED)\n",
        "\n",
        "# Create features directory\n",
        "os.makedirs(CONFIG['features_dir'], exist_ok=True)\n",
        "\n",
        "print(f\"✅ Configuration set for {DATASET_CHOICE.upper()} dataset!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXcwqqlp9HGo"
      },
      "source": [
        "## Cell 3: Dataset Download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvs4_8AS9HGo",
        "outputId": "0731d625-1e02-4eb8-e016-89df679d3636"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading lc25000 dataset: https://www.kaggle.com/datasets/javaidahmadwani/lc25000\n",
            "Dataset URL: https://www.kaggle.com/datasets/javaidahmadwani/lc25000\n",
            "Downloading lc25000.zip to ./lc25000-datasets/lc25000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.76G/1.76G [00:20<00:00, 93.1MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ LC25000 dataset downloaded successfully!\n"
          ]
        }
      ],
      "source": [
        "# Cell 3: Dataset Download\n",
        "# ========================\n",
        "def download_dataset():\n",
        "    \"\"\"Download dataset based on configuration\"\"\"\n",
        "    kaggle_credentials = {\"username\": \"YOUR_USERNAME\", \"key\": \"YOUR_KEY\"}\n",
        "\n",
        "    # ⚠️ REPLACE WITH YOUR ACTUAL KAGGLE CREDENTIALS ⚠️\n",
        "    kaggle_credentials = {\"username\": 'yuvanrajvengaladas', \"key\": '6fd3ece8111002ccca9494a6d7e6212e'}\n",
        "\n",
        "    with open(\"kaggle.json\", \"w\") as f:\n",
        "        json.dump(kaggle_credentials, f)\n",
        "\n",
        "    try:\n",
        "        with open(\"kaggle.json\", 'r') as f:\n",
        "            credentials = json.load(f)\n",
        "        os.environ['KAGGLE_USERNAME'] = credentials['username']\n",
        "        os.environ['KAGGLE_KEY'] = credentials['key']\n",
        "\n",
        "        print(f\"Downloading {DATASET_CHOICE} dataset: {CONFIG['dataset_url']}\")\n",
        "        od.download(CONFIG['dataset_url'], data_dir=CONFIG['download_folder'])\n",
        "        print(f\"✅ {DATASET_CHOICE.upper()} dataset downloaded successfully!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error downloading dataset: {e}\")\n",
        "        print(\"👉 Please ensure your kaggle.json file has correct credentials\")\n",
        "\n",
        "\n",
        "\n",
        "# Download the dataset\n",
        "\n",
        "if environment == \"colab\":\n",
        "  download_dataset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWtPpAaX9HGp"
      },
      "source": [
        "## Cell 4: Data Loading and Splitting Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rldg0n9v9HGp",
        "outputId": "6a24d190-a0c7-42e7-b4de-05bcf98d7209"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Data loading functions defined!\n"
          ]
        }
      ],
      "source": [
        "# Cell 4: Data Loading and Splitting Functions\n",
        "# ============================================\n",
        "def load_images_from_folder(folder, img_size=IMG_SIZE, max_samples=None):\n",
        "    \"\"\"Load images from folder structure (for pre-split datasets)\"\"\"\n",
        "    X, y, mapping = [], [], {}\n",
        "    class_folders = sorted([f for f in os.listdir(folder) if os.path.isdir(os.path.join(folder, f))])\n",
        "\n",
        "    for idx, cname in enumerate(class_folders):\n",
        "        mapping[idx] = cname\n",
        "        files = glob(os.path.join(folder, cname, \"*.jpg\")) + glob(os.path.join(folder, cname, \"*.png\")) + glob(os.path.join(folder, cname, \"*.jpeg\"))\n",
        "\n",
        "        if max_samples:\n",
        "            files = files[:max_samples]\n",
        "\n",
        "        for f in files:\n",
        "            img = cv2.imread(f)\n",
        "            if img is None:\n",
        "                continue\n",
        "            img = cv2.resize(img, img_size)\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "            X.append(img)\n",
        "            y.append(idx)\n",
        "\n",
        "    X = np.array(X)\n",
        "    y = np.array(y).reshape(-1, 1)\n",
        "    print(f\"✅ Loaded {len(X)} images from {folder}\")\n",
        "    return X, y, mapping\n",
        "\n",
        "def get_all_filepaths_and_labels(data_root):\n",
        "    \"\"\"Gather all image file paths and labels (for non-split datasets)\"\"\"\n",
        "    all_files, all_labels, mapping = [], [], {}\n",
        "    class_folders = sorted([f for f in os.listdir(data_root) if os.path.isdir(os.path.join(data_root, f))])\n",
        "\n",
        "    for idx, cname in enumerate(class_folders):\n",
        "        mapping[idx] = cname\n",
        "        class_dir = os.path.join(data_root, cname)\n",
        "        files = glob(os.path.join(class_dir, \"*.jpg\")) + glob(os.path.join(class_dir, \"*.png\"))\n",
        "        all_files.extend(files)\n",
        "        all_labels.extend([idx] * len(files))\n",
        "\n",
        "    print(f\"📁 Found {len(all_files)} images across {len(mapping)} classes.\")\n",
        "    return np.array(all_files), np.array(all_labels), mapping\n",
        "\n",
        "def load_images_from_paths(paths, labels, img_size=IMG_SIZE, max_samples=None):\n",
        "    \"\"\"Load images from file paths\"\"\"\n",
        "    X, valid_labels = [], []\n",
        "    if max_samples:\n",
        "        indices = np.random.choice(len(paths), min(max_samples, len(paths)), replace=False)\n",
        "        paths = paths[indices]\n",
        "        labels = labels[indices]\n",
        "\n",
        "    for i, f in enumerate(paths):\n",
        "        img = cv2.imread(f)\n",
        "        if img is None:\n",
        "            continue\n",
        "        img = cv2.resize(img, img_size)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        X.append(img)\n",
        "        valid_labels.append(labels[i])\n",
        "\n",
        "    X = np.array(X)\n",
        "    y = np.array(valid_labels).reshape(-1, 1)\n",
        "    print(f\"✅ Loaded {len(X)} images from paths.\")\n",
        "    return X, y\n",
        "\n",
        "def load_dataset_plants():\n",
        "    \"\"\"Load Plants dataset (pre-split)\"\"\"\n",
        "    X_train, y_train, mapping = load_images_from_folder(CONFIG['train_path'], max_samples=1000)\n",
        "    X_val, y_val, _ = load_images_from_folder(CONFIG['val_path'], max_samples=300)\n",
        "    X_test, y_test, _ = load_images_from_folder(CONFIG['test_path'], max_samples=300)\n",
        "    return X_train, y_train, X_val, y_val, X_test, y_test, mapping\n",
        "\n",
        "def load_dataset_aid():\n",
        "    \"\"\"Load AID dataset and split 70-10-20\"\"\"\n",
        "    X_paths, y_labels, mapping = get_all_filepaths_and_labels(CONFIG['data_root'])\n",
        "\n",
        "    # Split: 70% Train, 15% Validation, 15% Test\n",
        "    X_train_paths, X_temp_paths, y_train_labels, y_temp_labels = train_test_split(\n",
        "        X_paths, y_labels, test_size=0.3, random_state=RANDOM_SEED, stratify=y_labels\n",
        "    )\n",
        "    X_val_paths, X_test_paths, y_val_labels, y_test_labels = train_test_split(\n",
        "        X_temp_paths, y_temp_labels, test_size=0.5, random_state=RANDOM_SEED, stratify=y_temp_labels\n",
        "    )\n",
        "\n",
        "    # Load images\n",
        "    X_train, y_train = load_images_from_paths(X_train_paths, y_train_labels, max_samples=2000)\n",
        "    X_val, y_val = load_images_from_paths(X_val_paths, y_val_labels, max_samples=500)\n",
        "    X_test, y_test = load_images_from_paths(X_test_paths, y_test_labels, max_samples=500)\n",
        "\n",
        "    return X_train, y_train, X_val, y_val, X_test, y_test, mapping\n",
        "\n",
        "def load_dataset_lc25000():\n",
        "    \"\"\"Load LC25000 dataset with custom split\"\"\"\n",
        "    test_folder = os.path.join(CONFIG['data_root'], \"Test Set\")\n",
        "    train_val_folder = os.path.join(CONFIG['data_root'], \"Train and Validation Set\")\n",
        "\n",
        "    # Load test set\n",
        "    X_test, y_test, mapping = load_images_from_folder(test_folder, max_samples=500)\n",
        "\n",
        "    # Load train-val set and split 80-20\n",
        "    X_train_val, y_train_val, mapping = load_images_from_folder(train_val_folder, max_samples=2500)\n",
        "\n",
        "    # Split train-val into 80% train, 20% validation\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X_train_val, y_train_val, test_size=0.2, random_state=RANDOM_SEED, stratify=y_train_val\n",
        "    )\n",
        "\n",
        "    return X_train, y_train, X_val, y_val, X_test, y_test, mapping\n",
        "\n",
        "# Main data loading function\n",
        "def load_datasets():\n",
        "    \"\"\"Main function to load datasets based on choice\"\"\"\n",
        "    print(f\"📂 Loading {DATASET_CHOICE.upper()} dataset...\")\n",
        "\n",
        "    if CONFIG['split_type'] == \"pre_split\":\n",
        "        return load_dataset_plants()\n",
        "    elif CONFIG['split_type'] == \"70_10_20\":\n",
        "        return load_dataset_aid()\n",
        "    elif CONFIG['split_type'] == \"lc25000_split\":\n",
        "        return load_dataset_lc25000()\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown split type: {CONFIG['split_type']}\")\n",
        "\n",
        "print(\"✅ Data loading functions defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PR-CbZcG9HGq"
      },
      "source": [
        "## Cell 5: Feature Extraction Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZaQ-SKy9HGq",
        "outputId": "75de3e73-23d9-47e7-b52f-5b3474c673db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Feature extraction utilities defined!\n"
          ]
        }
      ],
      "source": [
        "# Cell 5: Feature Extraction Utilities\n",
        "# ====================================\n",
        "def preprocess_single_image_optimized(img):\n",
        "    \"\"\"Enhanced preprocessing for VGG16\"\"\"\n",
        "    resized_img = cv2.resize(img, VGG_TARGET_SIZE)\n",
        "    if random.random() > 0.5:\n",
        "        resized_img = cv2.flip(resized_img, 1)\n",
        "    processed_img = preprocess_input(resized_img.astype('float32'))\n",
        "    return processed_img\n",
        "\n",
        "def extract_features_optimized(X, dataset_name, batch_size=32):\n",
        "    \"\"\"Optimized feature extraction using VGG16\"\"\"\n",
        "    feature_file = os.path.join(CONFIG['features_dir'], f\"{dataset_name}_features.pkl\")\n",
        "\n",
        "    if os.path.exists(feature_file):\n",
        "        print(f\"📁 Loading cached features for {dataset_name}...\")\n",
        "        with open(feature_file, 'rb') as f:\n",
        "            return pickle.load(f)\n",
        "\n",
        "    print(f\"🔧 Extracting features for {dataset_name}...\")\n",
        "    base_model = VGG16(weights=\"imagenet\", include_top=False, input_shape=(224, 224, 3))\n",
        "    feature_extractor = tf.keras.Sequential([\n",
        "        base_model,\n",
        "        tf.keras.layers.GlobalAveragePooling2D()\n",
        "    ])\n",
        "\n",
        "    features = []\n",
        "    total_batches = (len(X) + batch_size - 1) // batch_size\n",
        "\n",
        "    for i in range(0, len(X), batch_size):\n",
        "        batch_images = []\n",
        "        for img in X[i:i+batch_size]:\n",
        "            processed_img = preprocess_single_image_optimized(img)\n",
        "            batch_images.append(processed_img)\n",
        "\n",
        "        batch = np.array(batch_images)\n",
        "        batch_features = feature_extractor.predict(batch, verbose=0)\n",
        "        features.append(batch_features)\n",
        "\n",
        "        # Progress tracking\n",
        "        batch_num = i // batch_size + 1\n",
        "        if batch_num % 10 == 0 or batch_num == total_batches:\n",
        "            print(f\"    Processed {batch_num}/{total_batches} batches\")\n",
        "\n",
        "        del batch, batch_images\n",
        "        gc.collect()\n",
        "\n",
        "    features = np.vstack(features)\n",
        "\n",
        "    # Save features\n",
        "    with open(feature_file, 'wb') as f:\n",
        "        pickle.dump(features, f)\n",
        "\n",
        "    # Cleanup\n",
        "    del feature_extractor, base_model\n",
        "    tf.keras.backend.clear_session()\n",
        "    gc.collect()\n",
        "\n",
        "    print(f\"💾 Saved features for {dataset_name}: {features.shape}\")\n",
        "    return features\n",
        "\n",
        "def load_and_extract_features():\n",
        "    \"\"\"Complete feature extraction pipeline\"\"\"\n",
        "    print(\"🚀 Starting feature extraction pipeline...\")\n",
        "\n",
        "    # Load datasets\n",
        "    X_train_img, y_train, X_val_img, y_val, X_test_img, y_test, mapping = load_datasets()\n",
        "\n",
        "    print(f\"🎯 Classes found: {len(mapping)}\")\n",
        "    print(f\"📊 Dataset sizes - Train: {X_train_img.shape[0]}, Val: {X_val_img.shape[0]}, Test: {X_test_img.shape[0]}\")\n",
        "\n",
        "    # Extract features\n",
        "    X_train_feats = extract_features_optimized(X_train_img, \"train\", batch_size=FEATURE_EXTRACTION_BATCH_SIZE)\n",
        "    X_val_feats = extract_features_optimized(X_val_img, \"val\", batch_size=FEATURE_EXTRACTION_BATCH_SIZE)\n",
        "    X_test_feats = extract_features_optimized(X_test_img, \"test\", batch_size=FEATURE_EXTRACTION_BATCH_SIZE)\n",
        "\n",
        "    # Clean up image data\n",
        "    del X_train_img, X_val_img, X_test_img\n",
        "    gc.collect()\n",
        "\n",
        "    print(\"✅ Feature extraction completed!\")\n",
        "    return X_train_feats, y_train, X_val_feats, y_val, X_test_feats, y_test, mapping\n",
        "\n",
        "print(\"✅ Feature extraction utilities defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MT8gSrp79HGr"
      },
      "source": [
        "## Cell 6: Structured Sparsity Norm (SSN) Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yxmSg10K9HGr",
        "outputId": "b631d572-54df-4a34-ccf4-532401444aa6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ SSN implementation complete!\n"
          ]
        }
      ],
      "source": [
        "# Cell 6: Structured Sparsity Norm (SSN) Implementation\n",
        "# ====================================================\n",
        "class StructuredSparsityNorm(tf.keras.regularizers.Regularizer):\n",
        "    def __init__(self, lambda1=1e-4, lambda2=1e-4):\n",
        "        self.lambda1 = lambda1\n",
        "        self.lambda2 = lambda2\n",
        "\n",
        "    def __call__(self, x):\n",
        "        l11_norm = self.lambda1 * tf.reduce_sum(tf.abs(x))\n",
        "        l2_norm = tf.sqrt(tf.reduce_sum(tf.square(x), axis=1) + 1e-8)\n",
        "        l21_norm = self.lambda2 * tf.reduce_sum(l2_norm)\n",
        "        return l11_norm + l21_norm\n",
        "\n",
        "    def get_config(self):\n",
        "        return {\"lambda1\": self.lambda1, \"lambda2\": self.lambda2}\n",
        "\n",
        "def build_ssn_perceptron(input_dim, n_classes, hidden_units=512, dropout_rate=0.5, lr=1e-3):\n",
        "    \"\"\"Build ANN with Structured Sparsity Norm regularization\"\"\"\n",
        "    tf.keras.backend.clear_session()\n",
        "    ssn_reg = StructuredSparsityNorm(lambda1=1e-4, lambda2=1e-4)\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(tf.keras.layers.Input(shape=(input_dim,)))\n",
        "    model.add(Dense(hidden_units, activation='relu', kernel_regularizer=ssn_reg))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(dropout_rate))\n",
        "    model.add(Dense(hidden_units//2, activation='relu', kernel_regularizer=ssn_reg))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "    model.add(Dense(n_classes, activation='softmax'))\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=lr),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "def evaluate_with_ssn(model, X_val, y_val):\n",
        "    \"\"\"Evaluate feature subset using Structured Sparsity Norm concept\"\"\"\n",
        "    preds_proba = model.predict(X_val, verbose=0)\n",
        "    preds = np.argmax(preds_proba, axis=1)\n",
        "    classification_error = 1 - accuracy_score(y_val.flatten(), preds)\n",
        "\n",
        "    # Get model weights for SSN evaluation\n",
        "    weights = []\n",
        "    for layer in model.layers:\n",
        "        if hasattr(layer, 'kernel') and layer.kernel is not None:\n",
        "            weights.append(layer.kernel.numpy())\n",
        "\n",
        "    if len(weights) > 0:\n",
        "        weight_matrix = weights[0]\n",
        "        l11_penalty = np.sum(np.abs(weight_matrix))\n",
        "        l21_penalty = np.sum(np.sqrt(np.sum(weight_matrix**2, axis=1)))\n",
        "        ssn_evaluation = classification_error + 1e-4 * (l11_penalty + l21_penalty)\n",
        "    else:\n",
        "        ssn_evaluation = classification_error\n",
        "\n",
        "    return ssn_evaluation, classification_error\n",
        "\n",
        "print(\"✅ SSN implementation complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgPE-pyZ9HGs"
      },
      "source": [
        "## Cell 7: GA-ANN-SSN Feature Selector Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSoRRIbb9HGs",
        "outputId": "4c05e722-78b9-4e9c-a8fb-9ad986fa15d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ GA-ANN-SSN Feature Selector class defined!\n"
          ]
        }
      ],
      "source": [
        "# Cell 7: GA-ANN-SSN Feature Selector Class\n",
        "# =========================================\n",
        "class GA_ANN_SSN_FeatureSelector:\n",
        "    def __init__(self, population_size=20, max_generations=100, crossover_rate=0.8, mutation_rate=0.2):\n",
        "        self.population_size = population_size\n",
        "        self.max_generations = max_generations\n",
        "        self.crossover_rate = crossover_rate\n",
        "        self.mutation_rate = mutation_rate\n",
        "\n",
        "    def initialize_population(self, n_features):\n",
        "        population = (np.random.rand(self.population_size, n_features) > 0.5).astype(int)\n",
        "        return population\n",
        "\n",
        "    def select_feature_subset(self, chromosome, X_data):\n",
        "        selected_indices = np.where(chromosome == 1)[0]\n",
        "        if len(selected_indices) == 0:\n",
        "            return X_data, np.arange(X_data.shape[1])\n",
        "        return X_data[:, selected_indices], selected_indices\n",
        "\n",
        "    def classify_with_perceptron(self, X_train, y_train, X_val, y_val, n_classes, selected_indices):\n",
        "        if len(selected_indices) == 0:\n",
        "            return None, 1.0\n",
        "\n",
        "        try:\n",
        "            model = build_ssn_perceptron(len(selected_indices), n_classes, hidden_units=256)\n",
        "            early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True, verbose=0)\n",
        "\n",
        "            history = model.fit(\n",
        "                X_train, y_train,\n",
        "                validation_data=(X_val, y_val),\n",
        "                epochs=EVAL_EPOCHS,\n",
        "                batch_size=32,\n",
        "                verbose=0,\n",
        "                callbacks=[early_stop]\n",
        "            )\n",
        "            return model, history\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Model training failed: {e}\")\n",
        "            return None, 1.0\n",
        "\n",
        "    def evaluate_with_ssn_norm(self, model, X_val, y_val, feature_ratio):\n",
        "        if model is None:\n",
        "            return 1.0\n",
        "\n",
        "        try:\n",
        "            ssn_score, classification_error = evaluate_with_ssn(model, X_val, y_val)\n",
        "            if DATASET_CHOICE == \"aid\":\n",
        "              fitness = 0.9 * classification_error + 0.1 * feature_ratio\n",
        "            else:\n",
        "              fitness = 0.7 * classification_error + 0.3 * feature_ratio\n",
        "            return fitness\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Evaluation failed: {e}\")\n",
        "            return 1.0\n",
        "\n",
        "    def select_best_population(self, population, fitness_values, method='roulette'):\n",
        "        if method == 'roulette':\n",
        "            fitness_array = np.array(fitness_values)\n",
        "            inverted_fitness = 1.0 / (1.0 + fitness_array)\n",
        "            probabilities = inverted_fitness / np.sum(inverted_fitness)\n",
        "            selected_indices = np.random.choice(len(population), size=len(population), p=probabilities)\n",
        "            return population[selected_indices]\n",
        "        else:\n",
        "            selected_population = []\n",
        "            for _ in range(len(population)):\n",
        "                idx1, idx2 = np.random.randint(0, len(population), 2)\n",
        "                if fitness_values[idx1] < fitness_values[idx2]:\n",
        "                    selected_population.append(population[idx1].copy())\n",
        "                else:\n",
        "                    selected_population.append(population[idx2].copy())\n",
        "            return np.array(selected_population)\n",
        "\n",
        "    def apply_crossover(self, population):\n",
        "        new_population = []\n",
        "        for i in range(0, len(population), 2):\n",
        "            if i + 1 < len(population):\n",
        "                parent1, parent2 = population[i], population[i + 1]\n",
        "                if np.random.rand() < self.crossover_rate:\n",
        "                    n_features = len(parent1)\n",
        "                    crossover_point = np.random.randint(1, n_features - 1)\n",
        "                    child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n",
        "                    child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n",
        "                else:\n",
        "                    child1, child2 = parent1.copy(), parent2.copy()\n",
        "                new_population.extend([child1, child2])\n",
        "\n",
        "        if len(new_population) < len(population):\n",
        "            new_population.append(population[-1].copy())\n",
        "        return np.array(new_population)\n",
        "\n",
        "    def apply_mutation(self, population):\n",
        "        mutated_population = []\n",
        "        for chromosome in population:\n",
        "            mutated_chromosome = chromosome.copy()\n",
        "            for i in range(len(mutated_chromosome)):\n",
        "                if np.random.rand() < self.mutation_rate:\n",
        "                    mutated_chromosome[i] = 1 - mutated_chromosome[i]\n",
        "            mutated_population.append(mutated_chromosome)\n",
        "        return np.array(mutated_population)\n",
        "\n",
        "    def optimize(self, X_train, y_train, X_val, y_val, n_classes, n_features):\n",
        "        print(\"🚀 Starting GA-ANN-SSN Feature Selection\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        population = self.initialize_population(n_features)\n",
        "        best_chromosome, best_fitness = None, float('inf')\n",
        "        best_accuracy, no_improvement_count = 0.0, 0\n",
        "\n",
        "        for generation in range(self.max_generations):\n",
        "            print(f\"\\n🔄 Generation {generation + 1}/{self.max_generations}\")\n",
        "            fitness_values, selected_features_list = [], []\n",
        "\n",
        "            for i, chromosome in enumerate(population):\n",
        "                X_train_subset, selected_indices = self.select_feature_subset(chromosome, X_train)\n",
        "                X_val_subset, _ = self.select_feature_subset(chromosome, X_val)\n",
        "                feature_ratio = len(selected_indices) / n_features\n",
        "\n",
        "                model, history = self.classify_with_perceptron(X_train_subset, y_train, X_val_subset, y_val, n_classes, selected_indices)\n",
        "                fitness = self.evaluate_with_ssn_norm(model, X_val_subset, y_val, feature_ratio)\n",
        "\n",
        "                fitness_values.append(fitness)\n",
        "                selected_features_list.append(selected_indices)\n",
        "\n",
        "                if fitness < best_fitness:\n",
        "                    best_fitness, best_chromosome = fitness, chromosome.copy()\n",
        "                    best_selected_features = selected_indices\n",
        "                    no_improvement_count = 0\n",
        "\n",
        "                    current_model = model\n",
        "                    if current_model is not None:\n",
        "                        try:\n",
        "                            preds = np.argmax(current_model.predict(X_val_subset, verbose=0), axis=1)\n",
        "                            best_accuracy = accuracy_score(y_val.flatten(), preds)\n",
        "                            print(f\"   ✅ New best! Fitness: {fitness:.4f}, Accuracy: {best_accuracy:.4f}, Features: {len(selected_indices)}\")\n",
        "                        except:\n",
        "                            print(f\"   ✅ New best! Fitness: {fitness:.4f}, Features: {len(selected_indices)} (accuracy calculation failed)\")\n",
        "                    else:\n",
        "                        print(f\"   ✅ New best! Fitness: {fitness:.4f}, Features: {len(selected_indices)} (no model)\")\n",
        "\n",
        "                if model is not None:\n",
        "                    del model\n",
        "                gc.collect()\n",
        "\n",
        "            no_improvement_count += 1\n",
        "            population_diversity = np.mean(np.std(population, axis=0))\n",
        "\n",
        "            # Check termination\n",
        "            if (generation >= self.max_generations or\n",
        "                no_improvement_count >= 30 or\n",
        "                population_diversity < 0.01):\n",
        "                break\n",
        "\n",
        "            selected_population = self.select_best_population(population, fitness_values)\n",
        "            crossed_population = self.apply_crossover(selected_population)\n",
        "            population = self.apply_mutation(crossed_population)\n",
        "\n",
        "            avg_fitness, current_best = np.mean(fitness_values), np.min(fitness_values)\n",
        "            print(f\"   📊 Stats - Best: {current_best:.4f}, Avg: {avg_fitness:.4f}, Diversity: {population_diversity:.4f}\")\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(\"🎯 GA-ANN-SSN COMPLETED\")\n",
        "        print(\"=\" * 60)\n",
        "        print(f\"Best Fitness: {best_fitness:.4f}\")\n",
        "        print(f\"Best Accuracy: {best_accuracy:.4f}\")\n",
        "        print(f\"Selected Features: {len(best_selected_features)}/{n_features}\")\n",
        "        print(f\"Final Generation: {generation + 1}\")\n",
        "\n",
        "        return best_chromosome, best_fitness, best_accuracy, best_selected_features\n",
        "\n",
        "print(\"✅ GA-ANN-SSN Feature Selector class defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9J_Zcb3f9HGt"
      },
      "source": [
        "## Cell 8: Main Pipeline Execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCczLcod9HGt",
        "outputId": "7c7f2312-52b9-403d-eeb2-4f175987b3d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Starting Unified GA-ANN-SSN Pipeline\n",
            "============================================================\n",
            "🚀 Starting feature extraction pipeline...\n",
            "📂 Loading LC25000 dataset...\n",
            "✅ Loaded 2499 images from ./lc25000-datasets/lc25000/lung_colon_image_set/Test Set\n",
            "✅ Loaded 12500 images from ./lc25000-datasets/lc25000/lung_colon_image_set/Train and Validation Set\n",
            "🎯 Classes found: 5\n",
            "📊 Dataset sizes - Train: 10000, Val: 2500, Test: 2499\n",
            "🔧 Extracting features for train...\n",
            "    Processed 10/79 batches\n",
            "    Processed 20/79 batches\n",
            "    Processed 30/79 batches\n",
            "    Processed 40/79 batches\n",
            "    Processed 50/79 batches\n",
            "    Processed 60/79 batches\n",
            "    Processed 70/79 batches\n",
            "    Processed 79/79 batches\n",
            "💾 Saved features for train: (10000, 512)\n",
            "🔧 Extracting features for val...\n",
            "    Processed 10/20 batches\n",
            "    Processed 20/20 batches\n",
            "💾 Saved features for val: (2500, 512)\n",
            "🔧 Extracting features for test...\n",
            "    Processed 10/20 batches\n",
            "    Processed 20/20 batches\n",
            "💾 Saved features for test: (2499, 512)\n",
            "✅ Feature extraction completed!\n",
            "🔧 Standardizing features...\n",
            "📊 Applying PCA...\n",
            "✅ PCA completed: 325 features (from 512)\n",
            "💾 Saved PCA and Scaler\n",
            "✅ Data preprocessing completed!\n"
          ]
        }
      ],
      "source": [
        "# Cell 8: Main Pipeline Execution\n",
        "# ===============================\n",
        "print(\"🚀 Starting Unified GA-ANN-SSN Pipeline\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Step 1: Load and extract features\n",
        "X_train_feats, y_train, X_val_feats, y_val, X_test_feats, y_test, mapping = load_and_extract_features()\n",
        "n_classes = len(mapping)\n",
        "\n",
        "# Step 2: Feature standardization\n",
        "print(\"🔧 Standardizing features...\")\n",
        "scaler = StandardScaler()\n",
        "X_train_feats = scaler.fit_transform(X_train_feats)\n",
        "X_val_feats = scaler.transform(X_val_feats)\n",
        "X_test_feats = scaler.transform(X_test_feats)\n",
        "\n",
        "# Step 3: Apply PCA\n",
        "print(\"📊 Applying PCA...\")\n",
        "pca_subset_size = min(2000, len(X_train_feats))\n",
        "pca_indices = np.random.choice(len(X_train_feats), pca_subset_size, replace=False)\n",
        "\n",
        "pca_temp = PCA(n_components=PCA_VARIANCE, random_state=RANDOM_SEED)\n",
        "pca_temp.fit(X_train_feats[pca_indices])\n",
        "n_components_95 = pca_temp.n_components_\n",
        "\n",
        "pca = PCA(n_components=n_components_95, random_state=RANDOM_SEED)\n",
        "pca.fit(X_train_feats[pca_indices])\n",
        "\n",
        "X_train_pca = pca.transform(X_train_feats)\n",
        "X_val_pca = pca.transform(X_val_feats)\n",
        "X_test_pca = pca.transform(X_test_feats)\n",
        "\n",
        "n_features = X_train_pca.shape[1]\n",
        "print(f\"✅ PCA completed: {n_features} features (from {X_train_feats.shape[1]})\")\n",
        "\n",
        "# Save PCA and scaler\n",
        "pca_file = os.path.join(CONFIG['features_dir'], \"pca_scaler.pkl\")\n",
        "with open(pca_file, 'wb') as f:\n",
        "    pickle.dump({'pca': pca, 'scaler': scaler}, f)\n",
        "print(\"💾 Saved PCA and Scaler\")\n",
        "\n",
        "print(\"✅ Data preprocessing completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KF_oyFFg9HGu"
      },
      "source": [
        "## Cell 9: EFFICIENT GA-ANN-SSN Feature Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "EaN497em9HGu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d2d218b-9126-4756-e7a9-e4324866bba6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Starting EFFICIENT GA-ANN-SSN Feature Selection...\n",
            "============================================================\n",
            "⚡ RUNNING IN EFFICIENT MODE ⚡\n",
            "   • Population: 8\n",
            "   • Generations: 10\n",
            "   • Evaluation Epochs: 3\n",
            "------------------------------------------------------------\n",
            "🚀 Starting GA-ANN-SSN Feature Selection\n",
            "============================================================\n",
            "\n",
            "🔄 Generation 1/10\n",
            "   ✅ New best! Fitness: 0.1993, Accuracy: 0.9460, Features: 175\n",
            "   ✅ New best! Fitness: 0.1838, Accuracy: 0.9484, Features: 160\n",
            "   ✅ New best! Fitness: 0.1804, Accuracy: 0.9480, Features: 156\n",
            "   📊 Stats - Best: 0.1804, Avg: 0.1995, Diversity: 0.4659\n",
            "\n",
            "🔄 Generation 2/10\n",
            "   ✅ New best! Fitness: 0.1787, Accuracy: 0.9596, Features: 163\n",
            "   ✅ New best! Fitness: 0.1752, Accuracy: 0.9700, Features: 167\n",
            "   ✅ New best! Fitness: 0.1637, Accuracy: 0.9640, Features: 150\n",
            "   📊 Stats - Best: 0.1637, Avg: 0.1884, Diversity: 0.4428\n",
            "\n",
            "🔄 Generation 3/10\n",
            "   ✅ New best! Fitness: 0.1629, Accuracy: 0.9624, Features: 148\n",
            "   📊 Stats - Best: 0.1629, Avg: 0.1950, Diversity: 0.4481\n",
            "\n",
            "🔄 Generation 4/10\n",
            "   📊 Stats - Best: 0.1705, Avg: 0.2005, Diversity: 0.4480\n",
            "\n",
            "🔄 Generation 5/10\n",
            "   📊 Stats - Best: 0.1807, Avg: 0.2028, Diversity: 0.4490\n",
            "\n",
            "🔄 Generation 6/10\n",
            "   📊 Stats - Best: 0.1719, Avg: 0.1859, Diversity: 0.4474\n",
            "\n",
            "🔄 Generation 7/10\n",
            "   📊 Stats - Best: 0.1666, Avg: 0.1972, Diversity: 0.4423\n",
            "\n",
            "🔄 Generation 8/10\n",
            "   📊 Stats - Best: 0.1740, Avg: 0.1966, Diversity: 0.4500\n",
            "\n",
            "🔄 Generation 9/10\n",
            "   📊 Stats - Best: 0.1782, Avg: 0.1872, Diversity: 0.4255\n",
            "\n",
            "🔄 Generation 10/10\n",
            "   📊 Stats - Best: 0.1785, Avg: 0.1984, Diversity: 0.4509\n",
            "\n",
            "============================================================\n",
            "🎯 GA-ANN-SSN COMPLETED\n",
            "============================================================\n",
            "Best Fitness: 0.1629\n",
            "Best Accuracy: 0.9624\n",
            "Selected Features: 148/325\n",
            "Final Generation: 10\n",
            "✅ GA-ANN-SSN feature selection completed!\n",
            "🎯 Final selection: 148 features out of 325\n",
            "\n",
            "ℹ️ Restored EVAL_EPOCHS to original value (10)\n"
          ]
        }
      ],
      "source": [
        "# Cell 9: EFFICIENT GA-ANN-SSN Feature Selection\n",
        "# ==============================================\n",
        "print(\"🚀 Starting EFFICIENT GA-ANN-SSN Feature Selection...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Efficient parameters for faster execution\n",
        "EFFICIENT_POPULATION_SIZE = 8\n",
        "EFFICIENT_MAX_GENERATION = 10\n",
        "EFFICIENT_EVAL_EPOCHS = 3\n",
        "EVAL_EPOCHS = EFFICIENT_EVAL_EPOCHS  # Override for efficiency\n",
        "\n",
        "print(f\"⚡ RUNNING IN EFFICIENT MODE ⚡\")\n",
        "print(f\"   • Population: {EFFICIENT_POPULATION_SIZE}\")\n",
        "print(f\"   • Generations: {EFFICIENT_MAX_GENERATION}\")\n",
        "print(f\"   • Evaluation Epochs: {EVAL_EPOCHS}\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "ga_selector = GA_ANN_SSN_FeatureSelector(\n",
        "    population_size=EFFICIENT_POPULATION_SIZE,\n",
        "    max_generations=EFFICIENT_MAX_GENERATION,\n",
        "    crossover_rate=GA_CROSSOVER_RATE,\n",
        "    mutation_rate=GA_MUTATION_RATE\n",
        ")\n",
        "\n",
        "try:\n",
        "    best_mask, best_fitness, best_accuracy, selected_indices = ga_selector.optimize(\n",
        "        X_train_pca, y_train, X_val_pca, y_val, n_classes, n_features\n",
        "    )\n",
        "    print(\"✅ GA-ANN-SSN feature selection completed!\")\n",
        "\n",
        "except UnboundLocalError as e:\n",
        "    print(f\"❌ Error in GA optimization: {e}\")\n",
        "    print(\"🔄 Creating fallback solution...\")\n",
        "    from sklearn.feature_selection import VarianceThreshold\n",
        "    selector = VarianceThreshold()\n",
        "    selector.fit(X_train_pca)\n",
        "    variances = selector.variances_\n",
        "    n_select = max(1, n_features // 2)\n",
        "    selected_indices = np.argsort(variances)[-n_select:]\n",
        "    best_mask = np.zeros(n_features)\n",
        "    best_mask[selected_indices] = 1\n",
        "    best_fitness, best_accuracy = 0.5, 0.5\n",
        "    print(f\"🔄 Using fallback: {len(selected_indices)} features selected\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"❌ Unexpected error: {e}\")\n",
        "    print(\"🔄 Creating random feature selection as fallback...\")\n",
        "    n_select = max(1, n_features // 2)\n",
        "    selected_indices = np.random.choice(n_features, n_select, replace=False)\n",
        "    best_mask = np.zeros(n_features)\n",
        "    best_mask[selected_indices] = 1\n",
        "    best_fitness, best_accuracy = 0.5, 0.5\n",
        "    print(f\"🔄 Using random selection: {len(selected_indices)} features selected\")\n",
        "\n",
        "print(f\"🎯 Final selection: {len(selected_indices)} features out of {n_features}\")\n",
        "\n",
        "# Restore original value\n",
        "EVAL_EPOCHS = 10\n",
        "print(f\"\\nℹ️ Restored EVAL_EPOCHS to original value ({EVAL_EPOCHS})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2OdUlTZ9HGu"
      },
      "source": [
        "## Cell 10: Final Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "IxCL8zd39HGv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4934d712-b5cc-4f80-f705-bec238636566"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🎯 Training Final Model with Selected Features...\n",
            "============================================================\n",
            "📊 Final training set: (12500, 148)\n",
            "📊 Test set: (2499, 148)\n",
            "🔧 Selected features: 148\n",
            "🔧 Training final model on combined train+val data...\n",
            "Epoch 1/50\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 14ms/step - accuracy: 0.7806 - loss: 1.7036\n",
            "Epoch 2/50\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9341 - loss: 1.1103\n",
            "Epoch 3/50\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9496 - loss: 0.9753\n",
            "Epoch 4/50\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9608 - loss: 0.8603\n",
            "Epoch 5/50\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9652 - loss: 0.7573\n",
            "Epoch 6/50\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9685 - loss: 0.6630\n",
            "Epoch 7/50\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9718 - loss: 0.5811\n",
            "Epoch 8/50\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9753 - loss: 0.5086\n",
            "Epoch 9/50\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9725 - loss: 0.4604\n",
            "Epoch 10/50\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9768 - loss: 0.4214\n",
            "Epoch 11/50\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9740 - loss: 0.4013\n",
            "Epoch 12/50\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9768 - loss: 0.3791\n",
            "Epoch 13/50\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9775 - loss: 0.3553\n",
            "Epoch 14/50\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9742 - loss: 0.3654\n",
            "Epoch 15/50\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9758 - loss: 0.3594\n",
            "Epoch 16/50\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9787 - loss: 0.3424\n",
            "Epoch 17/50\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9779 - loss: 0.3464\n",
            "Epoch 18/50\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9717 - loss: 0.3686\n",
            "Epoch 19/50\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9789 - loss: 0.3357\n",
            "Epoch 20/50\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9802 - loss: 0.3248\n",
            "Epoch 21/50\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9802 - loss: 0.3315\n",
            "Epoch 22/50\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9785 - loss: 0.3298\n",
            "Epoch 23/50\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9805 - loss: 0.3236\n",
            "Epoch 24/50\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9810 - loss: 0.3162\n",
            "Epoch 25/50\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9791 - loss: 0.3148\n",
            "Epoch 26/50\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9814 - loss: 0.3107\n",
            "Epoch 27/50\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9784 - loss: 0.3238\n",
            "Epoch 28/50\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9826 - loss: 0.3088\n",
            "Epoch 29/50\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9811 - loss: 0.3126\n",
            "Epoch 30/50\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9822 - loss: 0.3069\n",
            "Epoch 31/50\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9793 - loss: 0.3153\n",
            "Epoch 32/50\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9832 - loss: 0.3089\n",
            "Epoch 33/50\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9834 - loss: 0.3035\n",
            "Epoch 34/50\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9807 - loss: 0.3226\n",
            "Epoch 35/50\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9839 - loss: 0.3025\n",
            "Epoch 36/50\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9816 - loss: 0.3064\n",
            "Epoch 37/50\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9832 - loss: 0.3004\n",
            "Epoch 38/50\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9835 - loss: 0.3126\n",
            "Epoch 39/50\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9844 - loss: 0.2906\n",
            "Epoch 40/50\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9869 - loss: 0.2810\n",
            "Epoch 41/50\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9848 - loss: 0.2830\n",
            "Epoch 42/50\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9844 - loss: 0.2837\n",
            "Epoch 43/50\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9843 - loss: 0.2980\n",
            "Epoch 44/50\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9838 - loss: 0.3024\n",
            "Epoch 45/50\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9820 - loss: 0.3129\n",
            "Epoch 46/50\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9861 - loss: 0.2928\n",
            "Epoch 47/50\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9852 - loss: 0.2828\n",
            "Epoch 48/50\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9835 - loss: 0.2813\n",
            "Epoch 49/50\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9841 - loss: 0.2813\n",
            "Epoch 50/50\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9845 - loss: 0.2824\n",
            "✅ Final model training completed!\n"
          ]
        }
      ],
      "source": [
        "# Cell 10: Final Model Training\n",
        "# =============================\n",
        "print(\"\\n🎯 Training Final Model with Selected Features...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Prepare data with selected features\n",
        "X_train_final = np.vstack([X_train_pca, X_val_pca])[:, selected_indices]\n",
        "y_train_final = np.vstack([y_train, y_val])\n",
        "X_test_final = X_test_pca[:, selected_indices]\n",
        "\n",
        "print(f\"📊 Final training set: {X_train_final.shape}\")\n",
        "print(f\"📊 Test set: {X_test_final.shape}\")\n",
        "print(f\"🔧 Selected features: {X_train_final.shape[1]}\")\n",
        "\n",
        "# Build and train final model\n",
        "final_model = build_ssn_perceptron(X_train_final.shape[1], n_classes, hidden_units=512, dropout_rate=0.5)\n",
        "\n",
        "print(\"🔧 Training final model on combined train+val data...\")\n",
        "history = final_model.fit(\n",
        "    X_train_final, y_train_final,\n",
        "    epochs=FINAL_EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    verbose=1,\n",
        "    validation_split=0.0  # No validation during final training\n",
        ")\n",
        "\n",
        "print(\"✅ Final model training completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPD0OJzH9HGv"
      },
      "source": [
        "## Cell 11: Evaluation and Results Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JMLZ80129HGv"
      },
      "outputs": [],
      "source": [
        "# Cell 11: Evaluation and Results Summary\n",
        "# =======================================\n",
        "print(\"\\n📈 Evaluating on completely held-out TEST set...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Validation performance for reference\n",
        "X_val_selected = X_val_pca[:, selected_indices]\n",
        "val_preds = np.argmax(final_model.predict(X_val_selected, verbose=0), axis=1)\n",
        "val_accuracy = accuracy_score(y_val.flatten(), val_preds)\n",
        "\n",
        "# Test performance (true performance)\n",
        "test_preds = np.argmax(final_model.predict(X_test_final, verbose=0), axis=1)\n",
        "test_accuracy = accuracy_score(y_test.flatten(), test_preds)\n",
        "\n",
        "print(f\"🏆 VALIDATION Accuracy (for reference): {val_accuracy:.4f}\")\n",
        "print(f\"🎯 TEST Accuracy (true performance): {test_accuracy:.4f}\")\n",
        "\n",
        "# Final results summary\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(f\"🎯 FINAL GA-ANN-SSN RESULTS - {DATASET_CHOICE.upper()}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(f\"📊 Dataset Usage:\")\n",
        "print(f\"   • Train: {X_train_pca.shape[0]} samples (feature selection)\")\n",
        "print(f\"   • Validation: {X_val_pca.shape[0]} samples (fitness evaluation)\")\n",
        "print(f\"   • Test: {X_test_pca.shape[0]} samples (FINAL evaluation)\")\n",
        "print(f\"   • Final Training: {X_train_final.shape[0]} samples (train+val)\")\n",
        "\n",
        "print(f\"\\n🎯 Performance Metrics:\")\n",
        "print(f\"   • Best Fitness: {best_fitness:.4f}\")\n",
        "print(f\"   • Validation Accuracy: {val_accuracy:.4f}\")\n",
        "print(f\"   • 🏆 TEST Accuracy: {test_accuracy:.4f} ← TRUE PERFORMANCE\")\n",
        "\n",
        "print(f\"\\n🔧 Feature Selection:\")\n",
        "print(f\"   • Selected Features: {len(selected_indices)}/{n_features}\")\n",
        "print(f\"   • Feature Reduction: {((n_features - len(selected_indices)) / n_features * 100):.1f}%\")\n",
        "\n",
        "# Save results\n",
        "results = {\n",
        "    'dataset': DATASET_CHOICE,\n",
        "    'test_accuracy': test_accuracy,\n",
        "    'validation_accuracy': val_accuracy,\n",
        "    'ga_best_fitness': best_fitness,\n",
        "    'selected_features_count': len(selected_indices),\n",
        "    'total_features': n_features,\n",
        "    'selected_indices': selected_indices,\n",
        "    'dataset_sizes': {\n",
        "        'train': X_train_pca.shape[0],\n",
        "        'validation': X_val_pca.shape[0],\n",
        "        'test': X_test_pca.shape[0],\n",
        "        'final_training': X_train_final.shape[0]\n",
        "    },\n",
        "    'classes': mapping\n",
        "}\n",
        "\n",
        "results_file = os.path.join(CONFIG['features_dir'], \"ga_ann_ssn_results.pkl\")\n",
        "with open(results_file, 'wb') as f:\n",
        "    pickle.dump(results, f)\n",
        "\n",
        "print(f\"\\n💾 Results saved to: {results_file}\")\n",
        "print(\"✅ Unified pipeline completed successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "WpNEbroVHrGa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bbd424d-3a4a-4f11-969f-972b1e1f23ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "📊 RESULTS CONSOLIDATION\n",
            "============================================================\n",
            "💾 Results appended to: ga_ann_ssn_datasets_results.json\n",
            "📋 Summary updated in: ga_ann_ssn_datasets_summary.txt\n",
            "\n",
            "📊 LC25000 RESULTS:\n",
            "--------------------------------------------------\n",
            "Test Accuracy:       0.9756\n",
            "Validation Accuracy: 0.9916\n",
            "Selected Features:   148/325\n",
            "Feature Reduction:   54.5%\n",
            "Best Fitness:        0.1629\n",
            "\n",
            "✅ All results consolidated successfully!\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Cell 12: Results Consolidation\n",
        "# ==============================\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"📊 RESULTS CONSOLIDATION\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "def save_consolidated_results(test_accuracy, val_accuracy, best_fitness, selected_indices,\n",
        "                             n_features, n_classes, mapping, X_train_pca, X_test_pca):\n",
        "    \"\"\"Save all results to consolidated files\"\"\"\n",
        "\n",
        "    # Create results entry for current dataset\n",
        "    results_entry = {\n",
        "        'dataset': DATASET_CHOICE,\n",
        "        'performance_metrics': {\n",
        "            'test_accuracy': float(test_accuracy),\n",
        "            'validation_accuracy': float(val_accuracy),\n",
        "            'best_fitness': float(best_fitness),\n",
        "            'performance_gap': float(test_accuracy - val_accuracy)\n",
        "        },\n",
        "        'feature_selection': {\n",
        "            'selected_features_count': len(selected_indices),\n",
        "            'total_features': n_features,\n",
        "            'feature_reduction_percentage': float((n_features - len(selected_indices)) / n_features * 100),\n",
        "            'selected_features_ratio': float(len(selected_indices) / n_features)\n",
        "        },\n",
        "        'dataset_info': {\n",
        "            'num_classes': n_classes,\n",
        "            'class_names': list(mapping.values()),\n",
        "            'training_samples': int(X_train_pca.shape[0]),\n",
        "            'test_samples': int(X_test_pca.shape[0])\n",
        "        },\n",
        "        'timestamp': str(pd.Timestamp.now()) if 'pd' in globals() else \"Not available\"\n",
        "    }\n",
        "\n",
        "    # 1. Append to main JSON results file\n",
        "    main_json_file = \"ga_ann_ssn_datasets_results.json\"\n",
        "\n",
        "    # Load existing results or create new\n",
        "    if os.path.exists(main_json_file):\n",
        "        with open(main_json_file, 'r') as f:\n",
        "            all_results = json.load(f)\n",
        "    else:\n",
        "        all_results = {}\n",
        "\n",
        "    # Add/update current dataset results\n",
        "    all_results[DATASET_CHOICE] = results_entry\n",
        "\n",
        "    # Save updated results\n",
        "    with open(main_json_file, 'w') as f:\n",
        "        json.dump(all_results, f, indent=4)\n",
        "\n",
        "    print(f\"💾 Results appended to: {main_json_file}\")\n",
        "\n",
        "    # 2. Update summary text file\n",
        "    summary_file = \"ga_ann_ssn_datasets_summary.txt\"\n",
        "\n",
        "    # Create or update summary\n",
        "    if os.path.exists(summary_file):\n",
        "        with open(summary_file, 'r') as f:\n",
        "            existing_content = f.read()\n",
        "    else:\n",
        "        existing_content = \"\"\n",
        "\n",
        "    with open(summary_file, 'w') as f:\n",
        "        f.write(\"=\" * 70 + \"\\n\")\n",
        "        f.write(\"GA-ANN-SSN ALL DATASETS SUMMARY\\n\")\n",
        "        f.write(\"=\" * 70 + \"\\n\\n\")\n",
        "\n",
        "        # Write header\n",
        "        f.write(f\"{'Dataset':<12} {'Test Acc':<10} {'Val Acc':<10} {'Features':<12} {'Reduction':<12} {'Classes':<8}\\n\")\n",
        "        f.write(\"-\" * 70 + \"\\n\")\n",
        "\n",
        "        # Write all dataset entries\n",
        "        for dataset_name, dataset_results in all_results.items():\n",
        "            perf = dataset_results['performance_metrics']\n",
        "            feat = dataset_results['feature_selection']\n",
        "            info = dataset_results['dataset_info']\n",
        "\n",
        "            f.write(f\"{dataset_name:<12} {perf['test_accuracy']:<10.4f} {perf['validation_accuracy']:<10.4f} \"\n",
        "                   f\"{feat['selected_features_count']}/{feat['total_features']:<12} \"\n",
        "                   f\"{feat['feature_reduction_percentage']:<12.1f}% \"\n",
        "                   f\"{info['num_classes']:<8}\\n\")\n",
        "\n",
        "        f.write(\"\\n\" + \"=\" * 70 + \"\\n\")\n",
        "        f.write(\"LATEST RUN DETAILS:\\n\")\n",
        "        f.write(\"=\" * 70 + \"\\n\\n\")\n",
        "\n",
        "        # Add details for current run\n",
        "        f.write(f\"Dataset: {DATASET_CHOICE}\\n\")\n",
        "        f.write(f\"Test Accuracy: {test_accuracy:.4f}\\n\")\n",
        "        f.write(f\"Validation Accuracy: {val_accuracy:.4f}\\n\")\n",
        "        f.write(f\"Selected Features: {len(selected_indices)}/{n_features}\\n\")\n",
        "        f.write(f\"Feature Reduction: {((n_features - len(selected_indices)) / n_features * 100):.1f}%\\n\")\n",
        "        f.write(f\"Number of Classes: {n_classes}\\n\")\n",
        "        f.write(f\"Best Fitness: {best_fitness:.4f}\\n\")\n",
        "        f.write(f\"Classes: {', '.join(list(mapping.values()))}\\n\")\n",
        "        f.write(f\"Timestamp: {results_entry['timestamp']}\\n\")\n",
        "\n",
        "    print(f\"📋 Summary updated in: {summary_file}\")\n",
        "\n",
        "    # Print current results to console\n",
        "    print(f\"\\n📊 {DATASET_CHOICE.upper()} RESULTS:\")\n",
        "    print(\"-\" * 50)\n",
        "    print(f\"Test Accuracy:       {test_accuracy:.4f}\")\n",
        "    print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
        "    print(f\"Selected Features:   {len(selected_indices)}/{n_features}\")\n",
        "    print(f\"Feature Reduction:   {((n_features - len(selected_indices)) / n_features * 100):.1f}%\")\n",
        "    print(f\"Best Fitness:        {best_fitness:.4f}\")\n",
        "\n",
        "    return results_entry\n",
        "\n",
        "# Execute results consolidation\n",
        "try:\n",
        "    consolidated_results = save_consolidated_results(\n",
        "        test_accuracy, val_accuracy, best_fitness, selected_indices,\n",
        "        n_features, n_classes, mapping, X_train_pca, X_test_pca\n",
        "    )\n",
        "    print(\"\\n✅ All results consolidated successfully!\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"⚠️ Error in consolidating results: {e}\")\n",
        "    print(\"Continuing with existing results...\")\n",
        "\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "bchQV4rJHsBo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fce7ebd-73a4-4532-8366-543a4e0463b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "📄 CONSOLIDATED RESULTS SUMMARY (TXT)\n",
            "============================================================\n",
            "======================================================================\n",
            "GA-ANN-SSN ALL DATASETS SUMMARY\n",
            "======================================================================\n",
            "\n",
            "Dataset      Test Acc   Val Acc    Features     Reduction    Classes \n",
            "----------------------------------------------------------------------\n",
            "plants       0.9246     0.9743     171/331          48.3        % 30      \n",
            "aid          0.7600     1.0000     160/327          51.1        % 30      \n",
            "lc25000      0.9756     0.9916     148/325          54.5        % 5       \n",
            "\n",
            "======================================================================\n",
            "LATEST RUN DETAILS:\n",
            "======================================================================\n",
            "\n",
            "Dataset: lc25000\n",
            "Test Accuracy: 0.9756\n",
            "Validation Accuracy: 0.9916\n",
            "Selected Features: 148/325\n",
            "Feature Reduction: 54.5%\n",
            "Number of Classes: 5\n",
            "Best Fitness: 0.1629\n",
            "Classes: colon_aca, colon_n, lung_aca, lung_n, lung_scc\n",
            "Timestamp: Not available\n",
            "\n",
            "\n",
            "============================================================\n",
            "💡 FULL JSON RESULTS (DETAIL)\n",
            "============================================================\n",
            "{\n",
            "    \"plants\": {\n",
            "        \"dataset\": \"plants\",\n",
            "        \"performance_metrics\": {\n",
            "            \"test_accuracy\": 0.9246164109406271,\n",
            "            \"validation_accuracy\": 0.9742574257425742,\n",
            "            \"best_fitness\": 0.2903644322136141,\n",
            "            \"performance_gap\": -0.04964101480194716\n",
            "        },\n",
            "        \"feature_selection\": {\n",
            "            \"selected_features_count\": 171,\n",
            "            \"total_features\": 331,\n",
            "            \"feature_reduction_percentage\": 48.338368580060425,\n",
            "            \"selected_features_ratio\": 0.5166163141993958\n",
            "        },\n",
            "        \"dataset_info\": {\n",
            "            \"num_classes\": 30,\n",
            "            \"class_names\": [\n",
            "                \"aloevera\",\n",
            "                \"banana\",\n",
            "                \"bilimbi\",\n",
            "                \"cantaloupe\",\n",
            "                \"cassava\",\n",
            "                \"coconut\",\n",
            "                \"corn\",\n",
            "                \"cucumber\",\n",
            "                \"curcuma\",\n",
            "                \"eggplant\",\n",
            "                \"galangal\",\n",
            "                \"ginger\",\n",
            "                \"guava\",\n",
            "                \"kale\",\n",
            "                \"longbeans\",\n",
            "                \"mango\",\n",
            "                \"melon\",\n",
            "                \"orange\",\n",
            "                \"paddy\",\n",
            "                \"papaya\",\n",
            "                \"peper chili\",\n",
            "                \"pineapple\",\n",
            "                \"pomelo\",\n",
            "                \"shallot\",\n",
            "                \"soybeans\",\n",
            "                \"spinach\",\n",
            "                \"sweet potatoes\",\n",
            "                \"tobacco\",\n",
            "                \"waterapple\",\n",
            "                \"watermelon\"\n",
            "            ],\n",
            "            \"training_samples\": 23972,\n",
            "            \"test_samples\": 2998\n",
            "        },\n",
            "        \"timestamp\": \"Not available\"\n",
            "    },\n",
            "    \"aid\": {\n",
            "        \"dataset\": \"aid\",\n",
            "        \"performance_metrics\": {\n",
            "            \"test_accuracy\": 0.76,\n",
            "            \"validation_accuracy\": 1.0,\n",
            "            \"best_fitness\": 0.3927296636085627,\n",
            "            \"performance_gap\": -0.24\n",
            "        },\n",
            "        \"feature_selection\": {\n",
            "            \"selected_features_count\": 160,\n",
            "            \"total_features\": 327,\n",
            "            \"feature_reduction_percentage\": 51.07033639143731,\n",
            "            \"selected_features_ratio\": 0.4892966360856269\n",
            "        },\n",
            "        \"dataset_info\": {\n",
            "            \"num_classes\": 30,\n",
            "            \"class_names\": [\n",
            "                \"Airport\",\n",
            "                \"BareLand\",\n",
            "                \"BaseballField\",\n",
            "                \"Beach\",\n",
            "                \"Bridge\",\n",
            "                \"Center\",\n",
            "                \"Church\",\n",
            "                \"Commercial\",\n",
            "                \"DenseResidential\",\n",
            "                \"Desert\",\n",
            "                \"Farmland\",\n",
            "                \"Forest\",\n",
            "                \"Industrial\",\n",
            "                \"Meadow\",\n",
            "                \"MediumResidential\",\n",
            "                \"Mountain\",\n",
            "                \"Park\",\n",
            "                \"Parking\",\n",
            "                \"Playground\",\n",
            "                \"Pond\",\n",
            "                \"Port\",\n",
            "                \"RailwayStation\",\n",
            "                \"Resort\",\n",
            "                \"River\",\n",
            "                \"School\",\n",
            "                \"SparseResidential\",\n",
            "                \"Square\",\n",
            "                \"Stadium\",\n",
            "                \"StorageTanks\",\n",
            "                \"Viaduct\"\n",
            "            ],\n",
            "            \"training_samples\": 2000,\n",
            "            \"test_samples\": 500\n",
            "        },\n",
            "        \"timestamp\": \"Not available\"\n",
            "    },\n",
            "    \"lc25000\": {\n",
            "        \"dataset\": \"lc25000\",\n",
            "        \"performance_metrics\": {\n",
            "            \"test_accuracy\": 0.9755902360944377,\n",
            "            \"validation_accuracy\": 0.9916,\n",
            "            \"best_fitness\": 0.1629353846153846,\n",
            "            \"performance_gap\": -0.016009763905562302\n",
            "        },\n",
            "        \"feature_selection\": {\n",
            "            \"selected_features_count\": 148,\n",
            "            \"total_features\": 325,\n",
            "            \"feature_reduction_percentage\": 54.46153846153846,\n",
            "            \"selected_features_ratio\": 0.4553846153846154\n",
            "        },\n",
            "        \"dataset_info\": {\n",
            "            \"num_classes\": 5,\n",
            "            \"class_names\": [\n",
            "                \"colon_aca\",\n",
            "                \"colon_n\",\n",
            "                \"lung_aca\",\n",
            "                \"lung_n\",\n",
            "                \"lung_scc\"\n",
            "            ],\n",
            "            \"training_samples\": 10000,\n",
            "            \"test_samples\": 2499\n",
            "        },\n",
            "        \"timestamp\": \"Not available\"\n",
            "    }\n",
            "}\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "# --- Configuration (Ensure these match your saved filenames) ---\n",
        "SUMMARY_FILE = \"ga_ann_ssn_datasets_summary.txt\"\n",
        "JSON_FILE = \"ga_ann_ssn_datasets_results.json\"\n",
        "# ----------------------------------------------------------------\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"📄 CONSOLIDATED RESULTS SUMMARY (TXT)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "if os.path.exists(SUMMARY_FILE):\n",
        "    with open(SUMMARY_FILE, 'r') as f:\n",
        "        summary_content = f.read()\n",
        "    print(summary_content)\n",
        "else:\n",
        "    print(f\"❌ Error: Summary file '{SUMMARY_FILE}' not found.\")\n",
        "    print(\"Please ensure the file was created in the correct directory.\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"💡 FULL JSON RESULTS (DETAIL)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "if os.path.exists(JSON_FILE):\n",
        "    with open(JSON_FILE, 'r') as f:\n",
        "        json_results = json.load(f)\n",
        "\n",
        "    # Use json.dumps for neat, formatted printing\n",
        "    print(json.dumps(json_results, indent=4))\n",
        "else:\n",
        "    print(f\"❌ Error: JSON file '{JSON_FILE}' not found.\")\n",
        "    print(\"Please ensure the file was created in the correct directory.\")\n",
        "\n",
        "print(\"=\"*60)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}